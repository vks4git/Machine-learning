{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Регрессии"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Линейная регрессия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь x — столбец переменных, y — столбец значений. Функция возвращает столбец коэффициентов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def linear_regression(x, y):\n",
    "    A = numpy.matrix([[1 for i in range(sample_length)]] + x).transpose()\n",
    "    y = numpy.matrix(y).transpose()\n",
    "    return ((A.transpose() * A) ** (-1)) * A.transpose() * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь алгоритму предлагается построить аппроксимирующую кривую к множеству точек, отстоящему от прямой y = 3.4x + 6 на +-3 по абсциссе. Как видно, он с этим справился!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy\n",
    "import pylab\n",
    "\n",
    "sample_length = 100\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    A = numpy.matrix([[1 for i in range(sample_length)]] + x).transpose()\n",
    "    y = numpy.matrix(y).transpose()\n",
    "    return ((A.transpose() * A) ** (-1)) * A.transpose() * y\n",
    "\n",
    "\n",
    "data_x = [[0.1 * i for i in range(sample_length)]]\n",
    "data_linear = [3.4 * data_x[0][i] + 6 + random.uniform(-3, 3) for i in range(sample_length)]\n",
    "vec = linear_regression(data_x, data_linear)  # should be close to 3.4 and 6\n",
    "data_linear_regression = [data_x[0][i] * vec.flat[1] + vec.flat[0] for i in range(sample_length)]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(data_x[0], data_linear, 'ro')\n",
    "pylab.plot(data_x[0], data_linear_regression, label=\"y = %sx + %s\" % (vec.flat[1], vec.flat[0]), linewidth=3)\n",
    "pylab.legend(loc='upper left', title=\"Regressions\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Функциональная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def functional_regression(x, y, functions):\n",
    "    matrix = [[f(xij) for xij in x[0]] for f in functions]\n",
    "    return linear_regression(matrix, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм должен аппроксимировать кривой, состоящей из некоторой линейной комбинации функций cos(x), 2^x, x^3, множество точек, принадлежащих области y = 15cos(x) + 0.1*2^x - 0.03*x^3 +- 3. Функция преобразует столбец переменных в обобщённую матрицу Вандермонда и вызывает на ней линейную регрессию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pylab\n",
    "import random\n",
    "import math\n",
    "\n",
    "sample_length = 100\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    A = numpy.matrix([[1 for i in range(sample_length)]] + x).transpose()\n",
    "    y = numpy.matrix(y).transpose()\n",
    "    return ((A.transpose() * A) ** (-1)) * A.transpose() * y\n",
    "\n",
    "\n",
    "def functional_regression(x, y, functions):\n",
    "    matrix = [[f(xij) for xij in x[0]] for f in functions]\n",
    "    return linear_regression(matrix, y)\n",
    "\n",
    "\n",
    "data_x = [[0.1 * i for i in range(sample_length)]]\n",
    "data_functional = [15 * math.cos(data_x[0][i]) + 0.1 * (2 ** data_x[0][i]) - 0.03 * (data_x[0][i] ** 3) +\n",
    "                   random.uniform(-3, 3) for i in range(sample_length)]\n",
    "\n",
    "vec = functional_regression(data_x, data_functional, [lambda x: math.cos(x), lambda x: 2 ** x, lambda x: x ** 3])\n",
    "data_functional_regression = [vec.flat[1] * math.cos(data_x[0][i]) + vec.flat[2] * (2 ** data_x[0][i]) +\n",
    "                              vec.flat[3] * data_x[0][i] ** 3 + vec.flat[0] for i in range(sample_length)]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(data_x[0], data_functional, 'ro')\n",
    "pylab.plot(data_x[0], data_functional_regression, label=\"y = %scos(x) + (%s)2^x + (%s)x^3 + (%s)\" % \n",
    "           (vec.flat[1], vec.flat[2], vec.flat[3], vec.flat[0]), linewidth=3)\n",
    "pylab.legend(loc='upper left', title=\"Regressions\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_exponent(i):\n",
    "    return lambda z: z ** i\n",
    "\n",
    "def polynomial_regression(x, y, n):\n",
    "    return functional_regression(x, y, [get_exponent(i) for i in range(1, n + 1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм полиномиальной регрессии определён как частный случай функциональной регрессии: он просто вызывает функциональную регрессию, передавая ей список степенных функций. В примере он должей приблизить полиномом множество точек из области y = x * (x - 15) * (x - 30) / 125 + 5 +- 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pylab\n",
    "import random\n",
    "import math\n",
    "\n",
    "sample_length = 300\n",
    "\n",
    "\n",
    "def get_exponent(i):\n",
    "    return lambda z: z ** i\n",
    "\n",
    "\n",
    "def linear_regression(x, y):\n",
    "    A = numpy.matrix([[1 for i in range(sample_length)]] + x).transpose()\n",
    "    y = numpy.matrix(y).transpose()\n",
    "    return ((A.transpose() * A) ** (-1)) * A.transpose() * y\n",
    "\n",
    "\n",
    "def functional_regression(x, y, functions):\n",
    "    matrix = [[f(xij) for xij in x[0]] for f in functions]\n",
    "    return linear_regression(matrix, y)\n",
    "\n",
    "\n",
    "def polynomial_regression(x, y, n):\n",
    "    return functional_regression(x, y, [get_exponent(i) for i in range(1, n + 1)])\n",
    "\n",
    "\n",
    "data_x = [[0.1 * i for i in range(sample_length)]]\n",
    "data_cubic = [data_x[0][i] * (data_x[0][i] - 15) * (data_x[0][i] - 30) / 125 + 5 +\n",
    "              random.uniform(-3, 3) for i in range(sample_length)]\n",
    "\n",
    "vec = polynomial_regression(data_x, data_cubic, 3)  # should be close to 1/125, -9/25, 18/5 and 5\n",
    "data_polynomial_regression = [vec.flat[3] * data_x[0][i] ** 3 + vec.flat[2] * data_x[0][i] ** 2 +\n",
    "                              vec.flat[1] * data_x[0][i] + vec.flat[0] for i in range(sample_length)]\n",
    "\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(data_x[0], data_cubic, 'ro')\n",
    "pylab.plot(data_x[0], data_polynomial_regression, label=\"%sx^3 + (%s)x^2 + (%s)x + (%s)\" % \n",
    "           (vec.flat[3], vec.flat[2], vec.flat[1], vec.flat[0]), linewidth=3)\n",
    "pylab.legend(loc='upper left', title=\"Regressions\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "На предложенных датасетах были испытаны три алгоритма кросс-валидации:\n",
    "\n",
    "### K-fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold(k, data_x, data_y, regression):\n",
    "    error = 0.0\n",
    "    for i in range(k):\n",
    "        learning_x = []\n",
    "        learning_y = []\n",
    "        validation_x = []\n",
    "        validation_y = []\n",
    "        for j in range(len(data_y)):\n",
    "            if j // k == i:\n",
    "                validation_x.append(data_x[j])\n",
    "                validation_y.append(data_y[j])\n",
    "            else:\n",
    "                learning_x.append(data_x[j])\n",
    "                learning_y.append(data_y[j])\n",
    "        regression.learn(learning_x, learning_y)\n",
    "        error += regression.validate(validation_x, validation_y)\n",
    "    return error / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave-one-out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def leave_one_out(data_x, data_y, regression):\n",
    "    error = 0.0\n",
    "    for i in range(len(data_y)):\n",
    "        learning_x = []\n",
    "        learning_y = []\n",
    "        validation_x = []\n",
    "        validation_y = []\n",
    "        for j in range(len(data_y)):\n",
    "            if j == i:\n",
    "                validation_x.append(data_x[j])\n",
    "                validation_y.append(data_y[j])\n",
    "            else:\n",
    "                learning_x.append(data_x[j])\n",
    "                learning_y.append(data_y[j])\n",
    "        regression.learn(learning_x, learning_y)\n",
    "        error += regression.validate(validation_x, validation_y)\n",
    "    return error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte-carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def monte_carlo(n, k, data_x, data_y, regression):\n",
    "    error = 0.0\n",
    "    for i in range(k):\n",
    "        learning_x = []\n",
    "        learning_y = []\n",
    "        validation_x = []\n",
    "        validation_y = []\n",
    "        num = set()\n",
    "        for i in range(n):\n",
    "            p = 1\n",
    "            while p in num:\n",
    "                p = int(random.uniform(0, len(data_y) - 1))\n",
    "        for j in range(len(data_y)):\n",
    "            if j in num:\n",
    "                validation_x.append(data_x[j])\n",
    "                validation_y.append(data_y[j])\n",
    "            else:\n",
    "                learning_x.append(data_x[j])\n",
    "                learning_y.append(data_y[j])\n",
    "        regression.learn(learning_x, learning_y)\n",
    "        error += regression.validate(validation_x, validation_y)\n",
    "    return error / k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Первый датасет:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pickle\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_1.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x, y, \"ro\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 0.177619416999\n",
      "Leave-one-out CV: 0.0233138405295\n",
      "Monte Carlo CV: 0.248115176553\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_1.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "lr = LinearRegression()\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "lr.learn(x, y)\n",
    "vec = lr.get_coefficients()\n",
    "reg_y = [vec[0] * t + vec[1] for t in x[0]]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(x[0], reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, lr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, lr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, lr))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 0.178513866713\n",
      "Leave-one-out CV: 0.0234412675044\n",
      "Monte Carlo CV: 0.197291642336\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_1.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pr = PolynomialRegression(2)\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "pr.learn(x, y)\n",
    "vec = pr.get_coefficients()\n",
    "reg_y = [vec[0] * t ** 2 + vec[1] * t + vec[2] for t in x[0]]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(x[0], reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, pr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, pr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функциональная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 20.3673885244\n",
      "Leave-one-out CV: 2.07491307822\n",
      "Monte Carlo CV: 20.642683743\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_1.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "fr = FunctionalRegression([lambda z: math.cos(z), lambda z: math.cos(2 * z), lambda z: math.cos(3 * z)])\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "fr.learn(x, y)\n",
    "vec = fr.get_coefficients()\n",
    "reg_y = [vec[2] * math.cos(t) + vec[1] * math.cos(2 * t) + vec[0] * math.cos(3 * t) + vec[3] for t in sorted(x[0])]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(sorted(x[0]), reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, fr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, fr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Второй датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pickle\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_2.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x, y, \"ro\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 10.2866658414\n",
      "Leave-one-out CV: 1.05290592528\n",
      "Monte Carlo CV: 7.53338961384\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_2.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "lr = LinearRegression()\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "lr.learn(x, y)\n",
    "vec = lr.get_coefficients()\n",
    "reg_y = [vec[0] * t + vec[1] for t in x[0]]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(x[0], reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, lr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, lr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 0.996011586225\n",
      "Leave-one-out CV: 0.0934745843477\n",
      "Monte Carlo CV: 0.967113768942\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_2.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pr = PolynomialRegression(3)\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "pr.learn(x, y)\n",
    "vec = pr.get_coefficients()\n",
    "reg_y = [vec[0] * t ** 3 + vec[1] * t ** 2 + vec[2] * t + vec[3] for t in sorted(x[0])]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(sorted(x[0]), reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, pr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, pr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функциональная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 139.112681753\n",
      "Leave-one-out CV: 12.9076257059\n",
      "Monte Carlo CV: 114.760343647\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_2.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "fr = FunctionalRegression([lambda z: math.cos(z), lambda z: math.cos(2 * z), lambda z: math.cos(3 * z)])\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "fr.learn(x, y)\n",
    "vec = fr.get_coefficients()\n",
    "reg_y = [vec[2] * math.cos(t) + vec[1] * math.sin(t) + vec[0] * t + vec[3] for t in sorted(x[0])]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(sorted(x[0]), reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, fr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, fr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, fr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Третий датасет"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "import pickle\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_3.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x, y, \"ro\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Линейная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 41.5231773079\n",
      "Leave-one-out CV: 4.58407054879\n",
      "Monte Carlo CV: 48.1288490589\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_3.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "lr = LinearRegression()\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "lr.learn(x, y)\n",
    "vec = lr.get_coefficients()\n",
    "reg_y = [vec[0] * t + vec[1] for t in x[0]]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(x[0], reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, lr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, lr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, lr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Полиномиальная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 0.805339109221\n",
      "Leave-one-out CV: 0.0838836496424\n",
      "Monte Carlo CV: 0.749546753673\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_3.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "pr = PolynomialRegression(6)\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "pr.learn(x, y)\n",
    "vec = pr.get_coefficients()\n",
    "reg_y = [vec[0] * t ** 6 + vec[1] * t ** 5 + vec[2] * t ** 4 + vec[3] * t ** 3 + \n",
    "         vec[4] * t ** 2 + vec[5] * t + vec[6] for t in sorted(x[0])]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(sorted(x[0]), reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, pr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, pr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, pr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Функциональная регрессия"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K-fold CV: 0.60578910018\n",
      "Leave-one-out CV: 0.0733261610578\n",
      "Monte Carlo CV: 0.648908921202\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from cross_validation import *\n",
    "from regressions import *\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "x, y = pickle.load(open(\"/home/vks/Desktop/MachineLearning/task2/data/task2_dataset_3.txt\", \"rb\"), encoding=\"latin-1\")\n",
    "fr = FunctionalRegression([lambda z: math.cos(z), lambda z: math.sin(z), lambda z: z])\n",
    "x = [[t for t in numpy.matrix(x).transpose().flat]]\n",
    "y = [t for t in numpy.matrix(y).transpose().flat]\n",
    "fr.learn(x, y)\n",
    "vec = fr.get_coefficients()\n",
    "reg_y = [vec[2] * math.cos(t) + vec[1] * math.sin(t) + vec[0] * t + vec[3] for t in sorted(x[0])]\n",
    "\n",
    "pylab.xlabel(\"x\")\n",
    "pylab.ylabel(\"y\")\n",
    "pylab.plot(x[0], y, \"ro\")\n",
    "pylab.plot(sorted(x[0]), reg_y, linewidth=3)\n",
    "pylab.show()\n",
    "\n",
    "print(\"K-fold CV: %s\" % k_fold(10, x[0], y, fr))\n",
    "print(\"Leave-one-out CV: %s\" % leave_one_out(x[0], y, fr))\n",
    "print(\"Monte Carlo CV: %s\" % monte_carlo(10, 10, x[0], y, fr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
