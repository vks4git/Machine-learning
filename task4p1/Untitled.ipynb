{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree и Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Дерево принятия решений с CART "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основным элементом дерева является класс Node, который содержит в себе информацию о детях, глубине и типе узла, а также фичи и значения, по которым производится разделение."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Node:\n",
    "    feature = -1\n",
    "    key = -1\n",
    "    left = None\n",
    "    type = -1  # this field is assigned when node is a leaf; the type with the highest probability is selected.\n",
    "    # In case when multiple types have it, the first one is selected.\n",
    "    right = None\n",
    "    depth = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Дерево строится рекурсивно, в каждом узле вычисляется наиболее выгодное разделение; если таковое отсутствует, то узел становится листом, ему назначается тип согласно типам в обучающей выборке в нём.\n",
    "Классификация элемента производится аналогично: алгоритм рекурсивно спускается влево/вправо, пока не достигнет листа.\n",
    "Код в файле tree.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Строится список из нескольких деревьев (по умолчанию 10), каждое из которых обучается на некоторой доле фич (по умолчанию 25%). \n",
    "Код в файле random_forest.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Кросс-валидация (k-fold) проводилась на данных с ирисами Фишера. Оценивалось дерево на базе индека Джини и ансамбль из десяти деревьев, использующих подвыборку из 50% фич."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9400000000000001\n",
      "0.6733333333333332\n"
     ]
    }
   ],
   "source": [
    "from tree import Tree\n",
    "from random_forest import RandomForest\n",
    "from CV import k_fold\n",
    "import pickle\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "with open(\"iris.txt\", \"rb\") as f:\n",
    "    data, types = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "tree = Tree()\n",
    "forest = RandomForest(size=10, features=0.5)\n",
    "print(k_fold(10, data, types, tree))\n",
    "print(k_fold(10, data, types, forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Результат Random forest из sklearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.98039216  0.92156863  0.97916667]\n"
     ]
    }
   ],
   "source": [
    "from CV import k_fold\n",
    "import pickle\n",
    "from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "with open(\"iris.txt\", \"rb\") as f:\n",
    "    data, types = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "skl_forest = RFC(max_features=0.5)\n",
    "print(cross_val_score(skl_forest, data, types))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## grid search оптимальных параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trees : 1, features: 0.2, CV: 0.4666666666666666\n",
      "Trees : 1, features: 0.5, CV: 0.7066666666666667\n",
      "Trees : 1, features: 0.8, CV: 0.8333333333333333\n",
      "Trees : 2, features: 0.2, CV: 0.47333333333333333\n",
      "Trees : 2, features: 0.5, CV: 0.5533333333333333\n",
      "Trees : 2, features: 0.8, CV: 0.6133333333333335\n",
      "Trees : 3, features: 0.2, CV: 0.3333333333333333\n",
      "Trees : 3, features: 0.5, CV: 0.64\n",
      "Trees : 3, features: 0.8, CV: 0.9066666666666666\n",
      "Trees : 4, features: 0.2, CV: 0.27999999999999997\n",
      "Trees : 4, features: 0.5, CV: 0.8333333333333334\n",
      "Trees : 4, features: 0.8, CV: 0.8733333333333334\n",
      "Trees : 5, features: 0.2, CV: 0.4133333333333334\n",
      "Trees : 5, features: 0.5, CV: 0.74\n",
      "Trees : 5, features: 0.8, CV: 0.8866666666666667\n",
      "Trees : 6, features: 0.2, CV: 0.24666666666666667\n",
      "Trees : 6, features: 0.5, CV: 0.8666666666666668\n",
      "Trees : 6, features: 0.8, CV: 0.8733333333333334\n",
      "Trees : 7, features: 0.2, CV: 0.4866666666666667\n",
      "Trees : 7, features: 0.5, CV: 0.7533333333333334\n",
      "Trees : 7, features: 0.8, CV: 0.9066666666666668\n",
      "Trees : 8, features: 0.2, CV: 0.42000000000000004\n",
      "Trees : 8, features: 0.5, CV: 0.7799999999999999\n",
      "Trees : 8, features: 0.8, CV: 0.86\n",
      "Trees : 9, features: 0.2, CV: 0.42666666666666664\n",
      "Trees : 9, features: 0.5, CV: 0.7333333333333332\n",
      "Trees : 9, features: 0.8, CV: 0.9133333333333333\n",
      "Trees : 10, features: 0.2, CV: 0.38\n",
      "Trees : 10, features: 0.5, CV: 0.7933333333333332\n",
      "Trees : 10, features: 0.8, CV: 0.8533333333333333\n",
      "Trees : 11, features: 0.2, CV: 0.4133333333333333\n",
      "Trees : 11, features: 0.5, CV: 0.8133333333333332\n",
      "Trees : 11, features: 0.8, CV: 0.8933333333333333\n",
      "Trees : 12, features: 0.2, CV: 0.38\n",
      "Trees : 12, features: 0.5, CV: 0.8333333333333334\n",
      "Trees : 12, features: 0.8, CV: 0.9000000000000001\n",
      "Trees : 13, features: 0.2, CV: 0.5733333333333334\n",
      "Trees : 13, features: 0.5, CV: 0.8066666666666666\n",
      "Trees : 13, features: 0.8, CV: 0.8866666666666667\n",
      "Trees : 14, features: 0.2, CV: 0.52\n",
      "Trees : 14, features: 0.5, CV: 0.9066666666666668\n",
      "Trees : 14, features: 0.8, CV: 0.9\n",
      "Trees : 15, features: 0.2, CV: 0.38\n",
      "Trees : 15, features: 0.5, CV: 0.8466666666666667\n",
      "Trees : 15, features: 0.8, CV: 0.8800000000000001\n",
      "Trees : 16, features: 0.2, CV: 0.5333333333333333\n",
      "Trees : 16, features: 0.5, CV: 0.8533333333333333\n",
      "Trees : 16, features: 0.8, CV: 0.9266666666666665\n",
      "Trees : 17, features: 0.2, CV: 0.48\n",
      "Trees : 17, features: 0.5, CV: 0.78\n",
      "Trees : 17, features: 0.8, CV: 0.8933333333333333\n",
      "Trees : 18, features: 0.2, CV: 0.4600000000000001\n",
      "Trees : 18, features: 0.5, CV: 0.8733333333333333\n",
      "Trees : 18, features: 0.8, CV: 0.9066666666666668\n",
      "Trees : 19, features: 0.2, CV: 0.4066666666666666\n",
      "Trees : 19, features: 0.5, CV: 0.7466666666666666\n",
      "Trees : 19, features: 0.8, CV: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from random_forest import RandomForest\n",
    "from CV import k_fold\n",
    "import pickle\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "with open(\"iris.txt\", \"rb\") as f:\n",
    "    data, types = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "for k in range(1, 20):\n",
    "    for f in range(2, 11, 3):\n",
    "        forest = RandomForest(size=k, features=f / 10)\n",
    "        print(\"Trees : %s, features: %s, CV: %s\" % (k, f / 10, k_fold(10, data, types, forest)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно, оптимальным будет ансамбль из четырёх и больше деревьев, в каждом из которых используется 50-80% признаков."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Анализ алгоритма"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построение дерева займёт в худшем случае O(N) итераций, где N — размер датасета, в таком случае на каждой итерации данный будут разделяться на массив длины 1 и остальную часть датасета. Каждая итерация занимает O(F N^2) — поиск оптимального разбиения или поиск чаще всего встречающейся категории в случае листа, где F — число фичей. Суммарная сложность — O(F N^3).\n",
    "Классификация элемента займёт O(N) в худшем случае, т.к. дерево может быть несбалансированным.\n",
    "\n",
    "Построение ансамбля займёт в M раз больше времени, где M — его размер. Аналогично и с классификацией.\n",
    "\n",
    "Как видно из grid search, результаты в среднем чуть хуже, чем у kNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "from CV import k_fold\n",
    "import pickle\n",
    "from kNN import Naive_kNN\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "with open(\"iris.txt\", \"rb\") as f:\n",
    "    data, types = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "knn = Naive_kNN()\n",
    "print(k_fold(10, data, types, knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вероятно, это связано с довольно малым количеством фич: многие деревья получаются похожими, так как поднаборов признаков достаточно мало. К тому же, kNN обучается за O(N), поэтому при малом количестве фич, недостаточном для проявления \"проклятия размерности\", предпочтительно использовать его.\n",
    "\n",
    "Зависимость ошибки от количества используемых фич и величины ансамбля также прослеживается в результатах grid search: чем больше деревьев и фич, тем лучше. Однако качество резко возрастает только вначале, при количестве деревьев не больше шести, затем довольно слабо меняется, хотя и немного возрастает. Та же тенденция и в количестве фич: при 20% (одна фича в выборке с ирисами) алгоритм верно ответил в 20 — 50 процентах случаев. При 50% (две фичи) 50 — 90% верных ответов. При 80% фич (три фичи) верных ответов было 70 — 95%. \n",
    "\n",
    "Наконец, посмотрим, как поведёт себя алгоритм при экстремальных параметрах: 200 деревьев и 20% фич."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4133333333333334\n"
     ]
    }
   ],
   "source": [
    "from random_forest import RandomForest\n",
    "from CV import k_fold\n",
    "import pickle\n",
    "\n",
    "__author__ = 'vks'\n",
    "\n",
    "with open(\"iris.txt\", \"rb\") as f:\n",
    "    data, types = pickle.load(f, encoding=\"latin-1\")\n",
    "\n",
    "forest = RandomForest(size=200, features=0.2)\n",
    "print(k_fold(10, data, types, forest))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С всего лишь одной используемой фичей алгоритму не удалось добиться приемлемой точности даже при двухстах деревьев."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
